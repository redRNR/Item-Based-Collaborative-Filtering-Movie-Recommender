---
title: "Item-Based Collaborative Filtering Movie Recommendation System"
author: "Azra Park (3115735)"
date: "05/04/2022"
output:
  pdf_document:
    latex_engine: xelatex
  bookdown::pdf_document2: default
  word_document: default
bibliography: finalproj.bib
link-citations: yes
---
# Introduction
Recommendation systems are ML-based models that determine how similar users and products are to other things individual users and users like-them like, with the purpose of connecting them with recommendations among the plethora of selection in their given context. [@a2019_machine] In our given context, we will be looking at the dataset MovieLens in making a movie recommendation system, utilizing user ratings of various movies to then recommend the top movie recommendations for each user. To accomplish this, we will use item-based collaborative filtering in which given rating data by many users for items, the collaborative filtering algorithm finds similarity in the items based on the users’ ratings for the purposes of predicting missing ratings and creating a top-N recommendation list for a given user. [@sharma_2018_recommendersystems] Item-based collaborative filtering is a very popular method in recommendation systems first invented and implemented by Amazon in 1998. [@gupta_2020_itemtoitem]
The collaborative filtering algorithm we'll be using utilizes kNN regression techniques using cosine similarity to make these recommendations. \
\
In this project, we will provide a description of the data, the methodology behind the item-based collaborative filtering,  how the recommendation model is applied, and how we evaluate it. In the process we will build a movie recommendation system that will output a top 10 recommendation list for any given user.

# Methodology (Models and Methods)
The application of recommendation systems is heavily integrated into the function of many services we interact with today, be it Netflix with movies, Spotify with music, LinkedIn with people, or Amazon with products. Much of what you are viewing on these services is decided based on a recommendation system based on your activity and the activity of other users like you [@hahsler_2021_recommenderlab]. This is to say that recommendation systems often use real-time datasets produced by the company itself on users, items, descriptors, as well as any other variables that may have an effect on your decisions when using the service. As for recommendation system projects, popular datasets include MovieLens, the dataset we will be using for movie recommendation systems, Jester5k, that makes for a joke recommendation system, the Netflix Prize dataset, that was used by Netflix to increase their recommendation systems' accuracy by competition, and many more.

As a beginning to my project, I apply some exploratory analysis of the ratings and movies datasets. As a first step, I must create a sparse rating matrix (a matrix that has the majority of the elements equal to zero) for the purposes of later applying the collaborative filtering technique to fill in the matrix with predicted ratings from those given. From there, I create two heatmaps examining the cosine similarities between the first 20 users and then the first 20 movies in our newly created sparse rating matrix. This provides insight into how the collaborative filtering technique perceives similarities between different users and items.

![]("C:\Users\Asus\Desktop\20usersim.png" "a title")
![]("C:\Users\Asus\Desktop\20movsim.png" "a title")

More specifically, in item-based collaborative filtering, cosine-based similarity items are represented in a user vector-space. The angle between two of these vectors are computed, the cosine of the angle being the similarity between the items. Cosine-based similarity has one drawback in that it does not account for individual user’s rating biases as the calculations are performed over columns, with each column representing a different user. [@najafi_2016_evaluating]
\newpage

$$simil(x,y) = cos(\overrightarrow{x},\overrightarrow{y}) = \frac{\overrightarrow{x}\cdot\overrightarrow{y}}{||\overrightarrow{x}||\times||\overrightarrow{y}||}=\frac{\sum_{i\epsilon{I_{x,y}}}r_{x,i}r_{y,i}}{\sqrt{\sum_{i\epsilon{I_{x}}}r^2_{x,i}}\sqrt{\sum_{i\epsilon{I_{y}}}r^2_{y,i}}}$$
 After calculating the cosine similarity, each empty element in the matrix is then given a predicted rating for any user-item pair by way of weighted sum: 
 
$$P_{u,i} = \frac{\sum_{\text{all similar items}, N}(s_{i,N}*R_{u, N})}{\sum_{\text{all similar items}, N}(|s_{i,N}|)}$$

From all the items similar to our target item, we pick the items which an individual user has rated, then weight the user's rating for each of these items by the similarity between the target item and each of the similar items. Finally, the prediction is scaled by the sum of similarities to get the predicted ratings for each missing item, this done for each user filling in the initial sparse matrix. [@sarwar_2001_itembased; @algorithsm]

I then look at the unique ratings users are able to apply to movies, which range from 0.5 to 5, in 0.5 increments. Although 0 is included in the count, 0 is equivalent to no rating and is thus not included in the bar plot. A bar plot is used as a visual aid for comprehending each unique ratings’ total count found in the sparse rating matrix. This is then further supplemented by a histogram of each individual user's average rating, as a means of examining bias in the data.

![]("C:\Users\Asus\Desktop\countratval.png" "a title")
![]("C:\Users\Asus\Desktop\avgrat.png" "a title")

Now beyond exploratory data analysis, I begin optimizing for k, the number of nearest neighbours to each user in relation to their cosine similarities, with consideration to evaluation metrics in preparation for use in the recommendation system and its performance. In order to retain strong predictions, I first filter out users and movies that do not have a sufficient amount of data in order to ensure reliable predictions. Although this approach is not wholly practical, this does mitigate the cold start problem due to data sparsity often encountered in recommendation systems built on collaborative filtering. The cold start problem being that with users and items with little to no data points, it is difficult to accurately provide recommendations as collaborative filtering methods require a history of user preferences. My evaluation starts with creating an evaluation scheme that splits movie_ratings into a training set (80%) and a test set (20%). For the test set, all-but 4 items will be given to the recommendation algorithm; the algorithm sees all but 4 randomly selected withheld ratings for each test user and the algorithm is evaluated by how well it is able to predict the withheld items. This was chosen on the basis of optimal accuracy in consideration to the evaluation metrics. With using a realRatingMatrix, we are required to supply a goodRating threshold for which ratings are considered good for evaluation for each user. I decided to set this to 4.5, as to only capture the top recommendations for each user as a higher predicted rating makes for a "better" recommendation, as well as increasing the accuracy of the top-N list. Upon trying cross-validation and bootstrapping, I found them to be much slower and not as accurate as using the split method. Recommendation systems must be extremely time efficient due to the applications recommendation systems are used, such is the case with an ever updating log of user preferences and the requirement of having recommendations available while many users browse selection, this is not to say this performance cannot be achieved with cross-validation and bootstrapping, this is just my case in the implementation of this recommendation system. 

As a next step, I create a function to optimize for k (# of neighbours) that is applied in the process of when the recommendation model groups the k-most similar items based on cosine similarity for identifying user specific recommendations. In this function, we optimize k in consideration to the True Positive Rate (TPR) and False Positive Rate (FPR) of the top-N list of movies, as that is the end objective after all, accurate movie recommendations. TPR measures the percentage of actual positives that are accurately identified, while FPR measures the percentage of actual negatives incorrectly categorized as positive. A successful TPR value is still fairly low as recommendations are still difficult to predict due to the spectrum of user tastes and preferences, oftentimes a good value for TPR is as low as 0.08, the higher the TPR the better. However, this is not uncommon, we can simply consider how many items on Netflix, Amazon, or any other service utilizing recommendation systems we actually consume; most items that are recommended to us we do not consume. A good value for FPR is approximately 0.015, as this is to say there was a bad recommendation the user would not like in the top-N recommendation list, the lower the FPR the better.

$$TPR = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}$$
$$FPR = \frac{\text{False Positives}}{\text{True Negatives + False Positives}}$$

Upon finding the k value with the (relative) highest TPR and lowest FPR, we then use this k value in finding the Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) for our predicted ratings for which our recommendations are based on. RMSE and MAE are used for evaluating the predicted ratings against the actual user ratings in order to measure how accurate the filtering technique performs in filling-in the missing data [@doshi_2018_recommendation]. MAE computes the average of all the absolute value differences between the true and the predicted rating, while RMSE computes the square root of the mean value of all the squared differences between the true and predicted ratings [@najafi_2016_evaluating]. In both cases, the lower the error, the more accurately the recommendation engine predicts user ratings.

$$RMSE = \sqrt{\sum_{i=1}^{n}{\frac{(\hat{y_i} -y_i)^2}{n}}}$$

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y_i}|$$

An example of a successful test RMSE comes from the famous Netflix Prize in which Netflix had a grand prize of US$1,000,000, for any participating team that could improve the recommendation algorithm by 10%, or in other words that could achieve an RMSE of 0.8572 on the test set [@bennett_2007_the]. Though, in general, a successful test RMSE is approximately 0.9 with room to be higher than that, while a successful MAE is approximately 0.8, again with room beyond that. In order to make these calculations, we first use the predict() function to generate predictions for the known portion of the test data, then use the calcPredictAccuracy() function to calculate the error between the predictions and the unknown portions of the test data, with test RMSE, MSE, and MAE values being returned. [@topor_2017_rpubs] Another key point to mention is that for rating matrices it is important to normalize the ratings in order to remove the user rating bias. However, this is done by default in the function Recommender() that creates our model. Finally, I construct the function that predicts and returns the top 10 movie recommendations for any given user using the recommendation model we created before with the known test data.

# Data Description
I will be using a popular dataset used for Movie Recommendation Systems projects, MovieLens. Due to computing power limitations, a smaller subset of the larger dataset was used. This dataset contains 100,836 ratings applied to 9,742 movies by 610 users. The movies span across 19 genres and have 3,683 tags (descriptors) provided by the users in reviews, though these variables were not considered in the recommendation system as they were not applicable to the method of item-based collaborative filtering used. The MovieLens dataset is taken from GroupLens, a research lab in University of Minnesota’s Department of Computer Science and Engineering specializing in recommender systems, online communities, mobile and ubiquitous technologies, digital libraries, and local geographic information systems [@grouplens_2019_movielens]. The data is collected via the MovieLens website (http://movielens.org) that hosts hundreds of thousands of registered users in helping them find personalized movie recommendations.


# Model Fitting
During the process of fitting the model, there were a few procedures and decisions made in how to handle the data using statistical practices. For instance, one of the first requirements for processing the data was creating a sparse matrix that contained all the users and their ratings for all the movies which would be later filled in with predicted ratings from those given in the datasets. One of the most impactful changes in the data used in the recommendation system was the creation of the movie_ratings matrix that took the initial sparse rating matrix and filtered out any users and movies that did not meet the required amount of elements (set to 40) for the purposes of retaining reliable predictions. In creating an evaluation scheme for the recommendation system, I decided to choose the split method with an 80/20 split due to the required level of diversity between the train and test data in order to provide varied recommendations curated for the niche preferences of individuals; effect on evaluation metrics was also considered. I found using split to be the most accurate method, but also the fastest. This is important, as mentioned before recommendation systems must be extremely efficient in time and memory usage due to the scalability requirements of having recommendations readily available for millions of users. [@linden_2003_amazon] In the evaluation scheme, I also set the goodRating parameter to 4.5 for the model to be particularly discriminate in what makes a movie a personal recommendation rather than just a good movie. Then, set the given parameter to All-but-4 in order to maximize our evaluation metrics while still performing a comprehensive testing method in which the algorithm is evaluated on it's ability to predict the 4 withheld items. [@hahsler_2021_recommenderlab] I then built the IBCF_tune function to find the accuracy of the Top 10 recommendation list, to be implemented into topN_performance to find the optimal k that provided the highest TPR and lowest FPR for the most reliable recommendations. From this, I found that 5 was the optimal k to which I would use in the recommendation model and in finding the predicted ratings evaluation metrics, RMSE and MAE. This is important as the recommendation system is founded on the predicted ratings from collaborative filtering technique using cosine similarity that these metrics evaluate. I also set the Recommender() function to normalize the data by centering to remove users' rating biases.

# Results
I found the results of the recommendation system to be quite accurate with respect to the strong evaluation metrics, as well as the recommendations outputted for each user. I mention the recommendations themselves, as given my knowledge of movies, I was able to detect patterns in user preferences between movies and found cases in which movie series would have multiple movie entries, such as sequels, appear in the top 10 list which would be expected given the close cosine similarities. Although that is an example of mere heuristics, my evaluation metrics proved to be competitive with many of the projects and cases I had reviewed in researching this project. A good example of this is the Netflix Prize test RMSE of 0.8572, while I was able to achieve a test RMSE of 0.8652 and an MAE of 0.5980 in regards to my predicted ratings, where I found most reliable models using various datasets, including the one I had used, to produce approximately an MAE of 0.75. I also was able to achieve a strong TPR of 16.8% and FPR of 1.5% to which I found other examples to have a TPR closer to 9% and an FPR of 1.6% in regards to the accuracy of the outputted top 10 recommendations for each user (ie: my model has a probability of 16.8% to place an appealing movie in the top-10). [@cremonesi_2010_performance; @sarwar_2001_itembased]

![]("C:\Users\Asus\Desktop\TPRFPR.png" "a title")

I don't believe there is anything I would change to improve results, however I do sense that the parameters chosen may be too restrictive. For example, requiring users and movies to have at least 40 elements to be applied in the recommendation system or the setting of a high goodRating threshold parameter in the evaluation scheme, though this is not to say these measures did not have a beneficial purpose as they increased the accuracy of the model. I also felt the validation process was limited, but appropriate for the scope of the project, though I found each component of the recommendation system to have the effect I would expect especially seen in the choice of parameters. This is to say confidence in results are fairly low, but that is often the case with recommendation systems given the requirements of scalability in their applications with a trade-off between the level of prediction and speed, as well as the dubious task required of these algorithms in curating to the very subjective nature of user preferences, hence the expectation of a low TPR in our recommendations [@cremonesi_2010_performance; @couch_2020_tidytuesday].


# Conclusion
Overall, I am quite pleased with the results of my project, and more importantly I learned a lot. I find that my recommendation system not only works, but is also quite competitive in performance to others I had seen in the process. I found there were many limitations to this project, often simply due to the many special cases to consider much beyond the scope of the project. Some limitations include that in industry these algorithms are performed on a consistently updated dataset, often considering each individual session, as well as all past history which are implemented via data pipelines to automate these processes. Another limitation being that by requiring to have at least 40 data points for each movie and user as a means of retaining the quality of recommendations, we excluded a large selection of movies and users in the process. Though, it can be argued that there is something to be said for this approach as not only does it address the cold start problem, but there is a strong likelihood that movies with at least 40 watches would make for better recommendations anyway, given they are more popular than those excluded. With this in mind, one of the recommendation models often used functions on the basis of item popularity. In the same manner of movies and users being excluded due to requiring a certain amount of data, due to computing power limitations we were required to use a smaller subset of the larger MovieLens dataset, which restricts the movies included in the recommendation system, as well as reducing the potential accuracy of predictions due to having less data points. It is also important to consider that there are many approaches to creating a recommendation system, and it did not make sense to explore each caveat due to the time constraints and scope of the project. Be it simply using different recommendation models or using a larger set of variables for making recommendations such as with tags/descriptors, timestamps, etc. In closing, this project taught me the few key requirements that recommendation systems rely on, making for easy future reference and use. I believe it is an important piece of data science to learn, especially as these algorithms play a larger role in how society navigates life and makes decisions.

\newpage

# Appendix: R Code
```{r, warning = FALSE, message = FALSE}
library(recommenderlab)
library(tidyverse)
library(reshape2)
library(data.table)
```

### Exploratory Analysis
```{r}
set.seed(3)
movies_df <- read.csv('movies.csv')
ratings_df <- read.csv('ratings.csv')

head(movies_df)
head(ratings_df)

#Summary statistics
summary(movies_df)
summary(ratings_df)

#Creates sparse matrix for recommendation
rating_matrix <- reshape2::dcast(ratings_df, userId~movieId, value.var = 'rating', na.rm = FALSE)
rating_matrix <- as.matrix(rating_matrix)
rating_matrix <- as(rating_matrix[,2:9725], 'realRatingMatrix')

#User's Similarity
user_similarity_matrix <- similarity(rating_matrix[1:20, ], method = 'cosine', which = 'users')
image(as.matrix(user_similarity_matrix), main = 'First 20 Users\' Similarity') 

#Movie's Similarity
movie_similarity_matrix <- similarity(rating_matrix[,1:20], method = 'cosine', which = 'items')
image(as.matrix(movie_similarity_matrix), main = 'First 20 Movies\' Similarity')

#Rating values
rating_values <- as.vector(rating_matrix@data)
rating_table <- table(rating_values)
rating_table

ggplot(as.data.frame(rating_table[2:11]),aes(rating_values,Freq)) +
geom_bar(stat = "identity", fill = 'light blue') +
ggtitle('Count of Rating Values') +
geom_text(aes(label = signif(Freq)))

average_ratings <- rowMeans(rating_matrix)
qplot(average_ratings, fill = I('steelblue'), col = I('red')) +
  ggtitle('Average Rating by User')+
    ylab("Freq")
#We see a positive rating bias that will be accounted for in the Recommender model

#Filter Useful Data, using central limit theorem
movie_ratings <- rating_matrix[rowCounts(rating_matrix) >= 40,
                               colCounts(rating_matrix) >= 40]
```
[@a2019_machine;hahsler_2021_recommenderlab]

### Collaborative Filtering System
```{r}
set.seed(3)
recommendation_system <- recommenderRegistry$get_entries(dataType = 'realRatingMatrix')
#Default parameters
recommendation_system$IBCF_realRatingMatrix$parameters

model_evaluation <- evaluationScheme(movie_ratings, method="split",
                                     train=0.8, given=-4, goodRating=4.5)
```
[@hahsler_2021_recommenderlab]

### Optimization and Evaluation
```{r}
#Find optimal k
IBCF_tune <- function(evalScheme, parameters){
  IBCF_topN <- evaluate(model_evaluation, method = 'IBCF',
                        type = 'topNList', n = 10, param = list(k = parameters))
  IBCF_topN %>% 
    avg() %>% 
    as_tibble() %>% 
    mutate(param = parameters, model = 'IBCF') %>%
    return()
}
tune_grid <- tibble(parameters = c(1, 5, 10, 15, 20, 25, 30))
topN_performance <- tune_grid %>%
           mutate(results = map(parameters, ~IBCF_tune(model_evaluation, .x))) %>%
           unnest(cols = c(results))
topN_performance <- as.data.frame(topN_performance)
topN_performance

par(mfrow=c(1,2))
plot(topN_performance$parameters, topN_performance$TPR, type="b", col="red",
     xlab="k (# of Neighbours)", ylab="True Positive Rate (Sensitivity)")
abline()
plot(topN_performance$parameters, topN_performance$FPR, type = 'b', col="blue",
     xlab="k (# of Neighbours)", ylab="False Positive Rate (1 - Specificity)")

#With optimal k chosen, we can view our RMSE and MAE for the predicted ratings
recommend_model <- Recommender(getData(model_evaluation, "train"), "IBCF", 
                        param=list(normalize = 'center', method="Cosine", k = 5))

predictions <- predict(recommend_model, getData(model_evaluation, "known"), type="ratings")
error <- calcPredictionAccuracy(predictions, getData(model_evaluation, "unknown"))
error
```
[@couch_2020_tidytuesday; topor_2017_rpubs;hahsler_2021_recommenderlab]

### Recommendation system
```{r}
set.seed(3)
top_10 <- function(user){
  predicted_recommendations <- predict(object = recommend_model,
                                       newdata = getData(model_evaluation, "known"),
                                       n = 10)
  user_recommended_titles <- predicted_recommendations@itemLabels[predicted_recommendations@items[[user]]]
  user_recommended_titles_copy <- user_recommended_titles
  for (index in 1:10){
    user_recommended_titles_copy[index] <- as.character(subset(movies_df,
                                           movies_df$movieId == user_recommended_titles[index])$title)
  }
  user_recommended_titles_copy
}

#Top 10 recommendations for user 1
top_10(1)
```
\newpage

# References


